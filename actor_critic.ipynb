{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "# 環境構築\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "print( state_dim, act_dim )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 観測\n",
    "a_state = tf.placeholder(tf.float32, [None, state_dim])\n",
    "\n",
    "# Criticから渡される勾配\n",
    "a_action_grads = tf.placeholder(tf.float32, [None, act_dim])\n",
    "\n",
    "# ネットワーク構築\n",
    "with tf.variable_scope(\"actor\"):\n",
    "    fc1 = tf.contrib.layers.fully_connected(a_state, 400 )\n",
    "    fc2 = tf.contrib.layers.fully_connected(fc1, 300)\n",
    "    fc3 = tf.contrib.layers.fully_connected(fc2, act_dim, activation_fn=tf.tanh )\n",
    "    a_output = tf.multiply(fc3, 1)\n",
    "\n",
    "# 勾配を計算\n",
    "a_params = [ v for v in tf.trainable_variables() if \"actor\" in v.name ]\n",
    "a_grads = tf.gradients(a_output, a_params, -a_action_grads)\n",
    "a_grads = list(map(lambda x: tf.div(x, 20),  a_grads))\n",
    "\n",
    "# 最適化\n",
    "a_train_step = tf.train.AdamOptimizer(0.0001).apply_gradients(zip(a_grads,  a_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic\n",
    "c_state = tf.placeholder(tf.float32, [None, state_dim])\n",
    "c_action = tf.placeholder(tf.float32, [None, act_dim])\n",
    "c_target_q = tf.placeholder(tf.float32, [None, act_dim])\n",
    "\n",
    "fc1 = tf.contrib.layers.fully_connected(c_state, 400)\n",
    "w1 = tf.Variable(tf.random_normal([400, 300], mean=0.0, stddev=0.05))\n",
    "w2 = tf.Variable(tf.random_normal([act_dim, 300], mean=0.0, stddev=0.05))\n",
    "b2 = tf.Variable(tf.zeros([300]))\n",
    "\n",
    "# 二層目でactionと結合\n",
    "fc2 = tf.nn.relu(tf.matmul(fc1, w1) + tf.matmul(c_action, w2) + b2)\n",
    "c_output = tf.contrib.layers.fully_connected(fc2, act_dim, activation_fn=None)\n",
    "\n",
    "c_loss = tf.reduce_mean(tf.square(c_target_q - c_output))\n",
    "\n",
    "#c_loss = tf.losses.huber_loss(c_target_q, c_output)\n",
    "c_train_step = tf.train.AdamOptimizer(0.001).minimize(c_loss)\n",
    "c_action_grads = tf.gradients(c_output, c_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリ\n",
    "memory = deque(maxlen=1000)\n",
    "\n",
    "def sample( memory, batch_size ):\n",
    "    samp = random.sample( memory, batch_size )\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    \n",
    "    for s, a, r, ns in samp:\n",
    "        states.append( s )\n",
    "        actions.append( a )\n",
    "        rewards.append( r )\n",
    "        next_states.append( ns )\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "state, reward, done, _ = env.step(action)\n",
    "for i in range(1, 100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    memory.append((state, action, reward, next_state))\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -74.05612537251453 1.07291e-05\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run( tf.global_variables_initializer() )\n",
    "    for ep in range(1000):\n",
    "        total_reward = 0\n",
    "        env.reset()\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        for _ in range(1000):\n",
    "            # training\n",
    "            states, actions, rewards, next_states = sample( memory, 20 )\n",
    "                        \n",
    "            feed_dict = {a_state: states}\n",
    "            next_actions = sess.run( a_output, feed_dict=feed_dict )\n",
    "            next_q = sess.run( c_output, feed_dict={c_state:states, c_action: actions} )\n",
    "            \n",
    "            # critic学習\n",
    "            feed_dict = {\n",
    "                c_state: states, \n",
    "                c_action: actions, \n",
    "                c_target_q: rewards.reshape(20,1) + 0.99 * next_q\n",
    "            }\n",
    "            loss, q, _ = sess.run( [c_loss, c_output, c_train_step] , feed_dict=feed_dict)\n",
    "            \n",
    "            # criticでacitonの勾配を計算\n",
    "            feed_dict = {\n",
    "                c_state: states,\n",
    "                c_action: actions\n",
    "            }\n",
    "            action_grads = sess.run( c_action_grads, feed_dict=feed_dict )[0]\n",
    "            \n",
    "            # actor学習\n",
    "            feed_dict = {\n",
    "                a_state: states,\n",
    "                a_action_grads: action_grads                \n",
    "            }\n",
    "            sess.run( a_train_step, feed_dict=feed_dict )\n",
    "                        \n",
    "            # 実際に行動\n",
    "            if ep%10==0:\n",
    "                env.render()\n",
    "            action = sess.run( a_output, feed_dict={a_state: state.reshape(-1,state_dim)} )[0]\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            memory.append( (state.flatten(), action, reward, next_state) )\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        if ep%10==0:\n",
    "            print(ep  ,total_reward, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
